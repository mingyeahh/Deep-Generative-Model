{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from random import random\n",
    "from random import seed as r_seed\n",
    "import math\n",
    "from math import log2, floor\n",
    "from functools import lru_cache, partial\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import Adam\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import grad as torch_grad\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from kornia.filters import filter2d\n",
    "\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from adabelief_pytorch import AdaBelief\n",
    "\n",
    "from retry.api import retry_call\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# asserts\n",
    "\n",
    "assert torch.cuda.is_available(), 'You need to have an Nvidia GPU with CUDA installed.'\n",
    "\n",
    "# constants\n",
    "\n",
    "EXTS = ['jpg', 'jpeg', 'png', 'tiff']\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "@contextmanager\n",
    "def null_context():\n",
    "    yield\n",
    "\n",
    "def is_power_of_two(val):\n",
    "    return log2(val).is_integer()\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def set_requires_grad(model, bool):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = bool\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for i in iterable:\n",
    "            yield i\n",
    "\n",
    "def raise_if_nan(t):\n",
    "    if torch.isnan(t):\n",
    "        raise NanException\n",
    "\n",
    "def evaluate_in_chunks(max_batch_size, model, *args):\n",
    "    split_args = list(zip(*list(map(lambda x: x.split(max_batch_size, dim=0), args))))\n",
    "    chunked_outputs = [model(*i) for i in split_args]\n",
    "    if len(chunked_outputs) == 1:\n",
    "        return chunked_outputs[0]\n",
    "    return torch.cat(chunked_outputs, dim=0)\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    low_norm = low / torch.norm(low, dim=1, keepdim=True)\n",
    "    high_norm = high / torch.norm(high, dim=1, keepdim=True)\n",
    "    omega = torch.acos((low_norm * high_norm).sum(1))\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low + (torch.sin(val * omega) / so).unsqueeze(1) * high\n",
    "    return res\n",
    "\n",
    "def safe_div(n, d):\n",
    "    try:\n",
    "        res = n / d\n",
    "    except ZeroDivisionError:\n",
    "        prefix = '' if int(n >= 0) else '-'\n",
    "        res = float(f'{prefix}inf')\n",
    "    return res\n",
    "\n",
    "# loss functions\n",
    "\n",
    "def gen_hinge_loss(fake, real):\n",
    "    return fake.mean()\n",
    "\n",
    "def hinge_loss(real, fake):\n",
    "    return (F.relu(1 + real) + F.relu(1 - fake)).mean()\n",
    "\n",
    "def dual_contrastive_loss(real_logits, fake_logits):\n",
    "    device = real_logits.device\n",
    "    real_logits, fake_logits = map(lambda t: rearrange(t, '... -> (...)'), (real_logits, fake_logits))\n",
    "\n",
    "    def loss_half(t1, t2):\n",
    "        t1 = rearrange(t1, 'i -> i ()')\n",
    "        t2 = repeat(t2, 'j -> i j', i = t1.shape[0])\n",
    "        t = torch.cat((t1, t2), dim = -1)\n",
    "        return F.cross_entropy(t, torch.zeros(t1.shape[0], device = device, dtype = torch.long))\n",
    "\n",
    "    return loss_half(real_logits, fake_logits) + loss_half(-fake_logits, -real_logits)\n",
    "\n",
    "@lru_cache(maxsize=10)\n",
    "def det_randn(*args):\n",
    "    \"\"\"\n",
    "    deterministic random to track the same latent vars (and images) across training steps\n",
    "    helps to visualize same image over training steps\n",
    "    \"\"\"\n",
    "    return torch.randn(*args)\n",
    "\n",
    "def interpolate_between(a, b, *, num_samples, dim):\n",
    "    assert num_samples > 2\n",
    "    samples = []\n",
    "    step_size = 0\n",
    "    for _ in range(num_samples):\n",
    "        sample = torch.lerp(a, b, step_size)\n",
    "        samples.append(sample)\n",
    "        step_size += 1 / (num_samples - 1)\n",
    "    return torch.stack(samples, dim=dim)\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class NanException(Exception):\n",
    "    pass\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "    def update_average(self, old, new):\n",
    "        if not exists(old):\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "class ChanNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
    "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
    "        return (x - mean) / (var + self.eps).sqrt() * self.g + self.b\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = ChanNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "class SumBranches(nn.Module):\n",
    "    def __init__(self, branches):\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleList(branches)\n",
    "    def forward(self, x):\n",
    "        return sum(map(lambda fn: fn(x), self.branches))\n",
    "\n",
    "class Blur(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        f = torch.Tensor([1, 2, 1])\n",
    "        self.register_buffer('f', f)\n",
    "    def forward(self, x):\n",
    "        f = self.f\n",
    "        f = f[None, None, :] * f [None, :, None]\n",
    "        return filter2d(x, f, normalized=True)\n",
    "\n",
    "class Noise(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, noise = None):\n",
    "        b, _, h, w, device = *x.shape, x.device\n",
    "\n",
    "        if not exists(noise):\n",
    "            noise = torch.randn(b, 1, h, w, device = device)\n",
    "\n",
    "        return x + self.weight * noise\n",
    "\n",
    "def Conv2dSame(dim_in, dim_out, kernel_size, bias = True):\n",
    "    pad_left = kernel_size // 2\n",
    "    pad_right = (pad_left - 1) if (kernel_size % 2) == 0 else pad_left\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.ZeroPad2d((pad_left, pad_right, pad_left, pad_right)),\n",
    "        nn.Conv2d(dim_in, dim_out, kernel_size, bias = bias)\n",
    "    )\n",
    "\n",
    "# attention\n",
    "\n",
    "class DepthWiseConv2d(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, kernel_size, padding = 0, stride = 1, bias = True):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias = bias),\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, dim_head = 32, heads = 4, kernel_size = 3):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nonlin = nn.GELU()\n",
    "\n",
    "        self.to_lin_q = nn.Conv2d(dim, inner_dim, 1, bias = False)\n",
    "        self.to_lin_kv = DepthWiseConv2d(dim, inner_dim * 2, 3, padding = 1, bias = False)\n",
    "\n",
    "        self.to_q = nn.Conv2d(dim, inner_dim, 1, bias = False)\n",
    "        self.to_kv = nn.Conv2d(dim, inner_dim * 2, 1, bias = False)\n",
    "\n",
    "        self.to_out = nn.Conv2d(inner_dim * 2, dim, 1)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        h, x, y = self.heads, *fmap.shape[-2:]\n",
    "\n",
    "        # linear attention\n",
    "\n",
    "        lin_q, lin_k, lin_v = (self.to_lin_q(fmap), *self.to_lin_kv(fmap).chunk(2, dim = 1))\n",
    "        lin_q, lin_k, lin_v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = h), (lin_q, lin_k, lin_v))\n",
    "\n",
    "        lin_q = lin_q.softmax(dim = -1)\n",
    "        lin_k = lin_k.softmax(dim = -2)\n",
    "\n",
    "        lin_q = lin_q * self.scale\n",
    "\n",
    "        context = einsum('b n d, b n e -> b d e', lin_k, lin_v)\n",
    "        lin_out = einsum('b n d, b d e -> b n e', lin_q, context)\n",
    "        lin_out = rearrange(lin_out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n",
    "\n",
    "        # conv-like full attention\n",
    "\n",
    "        q, k, v = (self.to_q(fmap), *self.to_kv(fmap).chunk(2, dim = 1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) c x y', h = h), (q, k, v))\n",
    "\n",
    "        k = F.unfold(k, kernel_size = self.kernel_size, padding = self.kernel_size // 2)\n",
    "        v = F.unfold(v, kernel_size = self.kernel_size, padding = self.kernel_size // 2)\n",
    "\n",
    "        k, v = map(lambda t: rearrange(t, 'b (d j) n -> b n j d', d = self.dim_head), (k, v))\n",
    "\n",
    "        q = rearrange(q, 'b c ... -> b (...) c') * self.scale\n",
    "\n",
    "        sim = einsum('b i d, b i j d -> b i j', q, k)\n",
    "        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        full_out = einsum('b i j, b i j d -> b i d', attn, v)\n",
    "        full_out = rearrange(full_out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n",
    "\n",
    "        # add outputs of linear attention + conv like full attention\n",
    "\n",
    "        lin_out = self.nonlin(lin_out)\n",
    "        out = torch.cat((lin_out, full_out), dim = 1)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# dataset\n",
    "\n",
    "def convert_image_to(img_type, image):\n",
    "    if image.mode != img_type:\n",
    "        return image.convert(img_type)\n",
    "    return image\n",
    "\n",
    "class identity(object):\n",
    "    def __call__(self, tensor):\n",
    "        return tensor\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder,\n",
    "        image_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.folder = folder\n",
    "        self.image_size = image_size\n",
    "        self.paths = [p for ext in EXTS for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n",
    "        assert len(self.paths) > 0, f'No images were found in {folder} for training'\n",
    "\n",
    "        pillow_mode = 'RGB'\n",
    "\n",
    "        convert_image_fn = partial(convert_image_to, pillow_mode)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Lambda(convert_image_fn),\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(path)\n",
    "        return self.transform(img)\n",
    "\n",
    "# augmentations\n",
    "\n",
    "def random_hflip(tensor, prob):\n",
    "    if prob > random():\n",
    "        return tensor\n",
    "    return torch.flip(tensor, dims=(3,))\n",
    "\n",
    "class AugWrapper(nn.Module):\n",
    "    def __init__(self, D, image_size):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "\n",
    "    def forward(self, images, prob = 0., types = [], detach = False, **kwargs):\n",
    "        context = torch.no_grad if detach else null_context\n",
    "\n",
    "        with context():\n",
    "            if random() < prob:\n",
    "                images = random_hflip(images, prob=0.5)\n",
    "\n",
    "        return self.D(images, **kwargs)\n",
    "\n",
    "# modifiable global variables\n",
    "\n",
    "norm_class = nn.BatchNorm2d\n",
    "\n",
    "class PixelShuffleUpsample(nn.Module):\n",
    "    def __init__(self, dim, dim_out = None):\n",
    "        super().__init__()\n",
    "        dim_out = default(dim_out, dim)\n",
    "        conv = nn.Conv2d(dim, dim_out * 4, 1)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            conv,\n",
    "            nn.SiLU(),\n",
    "            nn.PixelShuffle(2)\n",
    "        )\n",
    "\n",
    "        self.init_conv_(conv)\n",
    "\n",
    "    def init_conv_(self, conv):\n",
    "        o, i, h, w = conv.weight.shape\n",
    "        conv_weight = torch.empty(o // 4, i, h, w)\n",
    "        nn.init.kaiming_uniform_(conv_weight)\n",
    "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n",
    "\n",
    "        conv.weight.data.copy_(conv_weight)\n",
    "        nn.init.zeros_(conv.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def SPConvDownsample(dim, dim_out = None):\n",
    "    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n",
    "    # named SP-conv in the paper, but basically a pixel unshuffle\n",
    "    dim_out = default(dim_out, dim)\n",
    "    return nn.Sequential(\n",
    "        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n",
    "        nn.Conv2d(dim * 4, dim_out, 1)\n",
    "    )\n",
    "\n",
    "# squeeze excitation classes\n",
    "\n",
    "# global context network\n",
    "# https://arxiv.org/abs/2012.13375\n",
    "# similar to squeeze-excite, but with a simplified attention pooling and a subsequent layer norm\n",
    "\n",
    "class GlobalContext(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        chan_in,\n",
    "        chan_out\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.to_k = nn.Conv2d(chan_in, 1, 1)\n",
    "        chan_intermediate = max(3, chan_out // 2)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(chan_in, chan_intermediate, 1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(chan_intermediate, chan_out, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        context = self.to_k(x)\n",
    "        context = context.flatten(2).softmax(dim = -1)\n",
    "        out = einsum('b i n, b c n -> b c i', context, x.flatten(2))\n",
    "        out = out.unsqueeze(-1)\n",
    "        return self.net(out)\n",
    "\n",
    "# frequency channel attention\n",
    "# https://arxiv.org/abs/2012.11879\n",
    "\n",
    "def get_1d_dct(i, freq, L):\n",
    "    result = math.cos(math.pi * freq * (i + 0.5) / L) / math.sqrt(L)\n",
    "    return result * (1 if freq == 0 else math.sqrt(2))\n",
    "\n",
    "def get_dct_weights(width, channel, fidx_u, fidx_v):\n",
    "    dct_weights = torch.zeros(1, channel, width, width)\n",
    "    c_part = channel // len(fidx_u)\n",
    "\n",
    "    for i, (u_x, v_y) in enumerate(zip(fidx_u, fidx_v)):\n",
    "        for x in range(width):\n",
    "            for y in range(width):\n",
    "                coor_value = get_1d_dct(x, u_x, width) * get_1d_dct(y, v_y, width)\n",
    "                dct_weights[:, i * c_part: (i + 1) * c_part, x, y] = coor_value\n",
    "\n",
    "    return dct_weights\n",
    "\n",
    "class FCANet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        chan_in,\n",
    "        chan_out,\n",
    "        reduction = 4,\n",
    "        width\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        freq_w, freq_h = ([0] * 8), list(range(8)) # in paper, it seems 16 frequencies was ideal\n",
    "        dct_weights = get_dct_weights(width, chan_in, [*freq_w, *freq_h], [*freq_h, *freq_w])\n",
    "        self.register_buffer('dct_weights', dct_weights)\n",
    "\n",
    "        chan_intermediate = max(3, chan_out // reduction)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(chan_in, chan_intermediate, 1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(chan_intermediate, chan_out, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = reduce(x * self.dct_weights, 'b c (h h1) (w w1) -> b c h1 w1', 'sum', h1 = 1, w1 = 1)\n",
    "        return self.net(x)\n",
    "\n",
    "# generative adversarial network\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        latent_dim = 256,\n",
    "        fmap_max = 256,\n",
    "        fmap_inverse_coef = 10,\n",
    "        attn_res_layers = [],\n",
    "        freq_chan_attn = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        resolution = log2(image_size)\n",
    "        assert is_power_of_two(image_size), 'image size must be a power of 2'\n",
    "\n",
    "        init_channel = 3\n",
    "\n",
    "        fmap_max = default(fmap_max, latent_dim)\n",
    "\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, latent_dim * 2, 4),\n",
    "            norm_class(latent_dim * 2),\n",
    "            nn.GLU(dim = 1)\n",
    "        )\n",
    "\n",
    "        num_layers = int(resolution) - 2\n",
    "        features = list(map(lambda n: (n,  2 ** (fmap_inverse_coef - n)), range(2, num_layers + 2)))\n",
    "        features = list(map(lambda n: (n[0], min(n[1], fmap_max)), features))\n",
    "        features = list(map(lambda n: 3 if n[0] >= 8 else n[1], features))\n",
    "        features = [latent_dim, *features]\n",
    "\n",
    "        in_out_features = list(zip(features[:-1], features[1:]))\n",
    "\n",
    "        self.res_layers = range(2, num_layers + 2)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.res_to_feature_map = dict(zip(self.res_layers, in_out_features))\n",
    "\n",
    "        self.sle_map = ((2, 5), (3, 6), (4, 7))\n",
    "        self.sle_map = list(filter(lambda t: t[0] <= resolution and t[1] <= resolution, self.sle_map))\n",
    "        self.sle_map = dict(self.sle_map)\n",
    "\n",
    "        self.num_layers_spatial_res = 1\n",
    "\n",
    "        for (res, (chan_in, chan_out)) in zip(self.res_layers, in_out_features):\n",
    "            image_width = 2 ** res\n",
    "\n",
    "            attn = None\n",
    "            if image_width in attn_res_layers:\n",
    "                attn = PreNorm(chan_in, LinearAttention(chan_in))\n",
    "\n",
    "            sle = None\n",
    "            if res in self.sle_map:\n",
    "                residual_layer = self.sle_map[res]\n",
    "                sle_chan_out = self.res_to_feature_map[residual_layer - 1][-1]\n",
    "\n",
    "                if freq_chan_attn:\n",
    "                    sle = FCANet(\n",
    "                        chan_in = chan_out,\n",
    "                        chan_out = sle_chan_out,\n",
    "                        width = 2 ** (res + 1)\n",
    "                    )\n",
    "                else:\n",
    "                    sle = GlobalContext(\n",
    "                        chan_in = chan_out,\n",
    "                        chan_out = sle_chan_out\n",
    "                    )\n",
    "\n",
    "            layer = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    PixelShuffleUpsample(chan_in),\n",
    "                    Blur(),\n",
    "                    Conv2dSame(chan_in, chan_out * 2, 4),\n",
    "                    Noise(),\n",
    "                    norm_class(chan_out * 2),\n",
    "                    nn.GLU(dim = 1)\n",
    "                ),\n",
    "                sle,\n",
    "                attn\n",
    "            ])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(features[-1], init_channel, 3, padding = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b c -> b c () ()')\n",
    "        x = self.initial_conv(x)\n",
    "        x = F.normalize(x, dim = 1)\n",
    "\n",
    "        residuals = dict()\n",
    "\n",
    "        for (res, (up, sle, attn)) in zip(self.res_layers, self.layers):\n",
    "            if exists(attn):\n",
    "                x = attn(x) + x\n",
    "\n",
    "            x = up(x)\n",
    "\n",
    "            if exists(sle):\n",
    "                out_res = self.sle_map[res]\n",
    "                residual = sle(x)\n",
    "                residuals[out_res] = residual\n",
    "\n",
    "            next_res = res + 1\n",
    "            if next_res in residuals:\n",
    "                x = x * residuals[next_res]\n",
    "\n",
    "        return self.out_conv(x)\n",
    "\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        chan_in,\n",
    "        chan_out = 3,\n",
    "        num_upsamples = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        final_chan = chan_out\n",
    "        chans = chan_in\n",
    "\n",
    "        for ind in range(num_upsamples):\n",
    "            last_layer = ind == (num_upsamples - 1)\n",
    "            chan_out = chans if not last_layer else final_chan * 2\n",
    "            layer = nn.Sequential(\n",
    "                PixelShuffleUpsample(chans),\n",
    "                nn.Conv2d(chans, chan_out, 3, padding = 1),\n",
    "                nn.GLU(dim = 1)\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "            chans //= 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        fmap_max = 256,\n",
    "        fmap_inverse_coef = 10,\n",
    "        disc_output_size = 5,\n",
    "        attn_res_layers = []\n",
    "    ):\n",
    "        super().__init__()\n",
    "        resolution = log2(image_size)\n",
    "        assert is_power_of_two(image_size), 'image size must be a power of 2'\n",
    "        assert disc_output_size in {1, 5}, 'discriminator output dimensions can only be 5x5 or 1x1'\n",
    "\n",
    "        resolution = int(resolution)\n",
    "\n",
    "        init_channel = 3\n",
    "\n",
    "        num_non_residual_layers = max(0, int(resolution) - 8)\n",
    "\n",
    "        non_residual_resolutions = range(min(8, resolution), 2, -1)\n",
    "        features = list(map(lambda n: (n,  2 ** (fmap_inverse_coef - n)), non_residual_resolutions))\n",
    "        features = list(map(lambda n: (n[0], min(n[1], fmap_max)), features))\n",
    "\n",
    "        if num_non_residual_layers == 0:\n",
    "            res, _ = features[0]\n",
    "            features[0] = (res, init_channel)\n",
    "\n",
    "        chan_in_out = list(zip(features[:-1], features[1:]))\n",
    "\n",
    "        self.non_residual_layers = nn.ModuleList([])\n",
    "        for ind in range(num_non_residual_layers):\n",
    "            last_layer = ind == (num_non_residual_layers - 1)\n",
    "            chan_out = features[0][-1] if last_layer else init_channel\n",
    "\n",
    "            self.non_residual_layers.append(nn.Sequential(\n",
    "                Blur(),\n",
    "                nn.Conv2d(init_channel, chan_out, 4, stride = 2, padding = 1),\n",
    "                nn.LeakyReLU(0.1)\n",
    "            ))\n",
    "\n",
    "        self.residual_layers = nn.ModuleList([])\n",
    "\n",
    "        for (res, ((_, chan_in), (_, chan_out))) in zip(non_residual_resolutions, chan_in_out):\n",
    "            image_width = 2 ** res\n",
    "\n",
    "            attn = None\n",
    "            if image_width in attn_res_layers:\n",
    "                attn = PreNorm(chan_in, LinearAttention(chan_in))\n",
    "\n",
    "            self.residual_layers.append(nn.ModuleList([\n",
    "                SumBranches([\n",
    "                    nn.Sequential(\n",
    "                        Blur(),\n",
    "                        SPConvDownsample(chan_in, chan_out),\n",
    "                        nn.LeakyReLU(0.1),\n",
    "                        nn.Conv2d(chan_out, chan_out, 3, padding = 1),\n",
    "                        nn.LeakyReLU(0.1)\n",
    "                    ),\n",
    "                    nn.Sequential(\n",
    "                        Blur(),\n",
    "                        nn.AvgPool2d(2),\n",
    "                        nn.Conv2d(chan_in, chan_out, 1),\n",
    "                        nn.LeakyReLU(0.1),\n",
    "                    )\n",
    "                ]),\n",
    "                attn\n",
    "            ]))\n",
    "\n",
    "        last_chan = features[-1][-1]\n",
    "        if disc_output_size == 5:\n",
    "            self.to_logits = nn.Sequential(\n",
    "                nn.Conv2d(last_chan, last_chan, 1),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Conv2d(last_chan, 1, 4)\n",
    "            )\n",
    "        elif disc_output_size == 1:\n",
    "            self.to_logits = nn.Sequential(\n",
    "                Blur(),\n",
    "                nn.Conv2d(last_chan, last_chan, 3, stride = 2, padding = 1),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Conv2d(last_chan, 1, 4)\n",
    "            )\n",
    "\n",
    "        self.to_shape_disc_out = nn.Sequential(\n",
    "            nn.Conv2d(init_channel, 64, 3, padding = 1),\n",
    "            Residual(PreNorm(64, LinearAttention(64))),\n",
    "            SumBranches([\n",
    "                nn.Sequential(\n",
    "                    Blur(),\n",
    "                    SPConvDownsample(64, 32),\n",
    "                    nn.LeakyReLU(0.1),\n",
    "                    nn.Conv2d(32, 32, 3, padding = 1),\n",
    "                    nn.LeakyReLU(0.1)\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    Blur(),\n",
    "                    nn.AvgPool2d(2),\n",
    "                    nn.Conv2d(64, 32, 1),\n",
    "                    nn.LeakyReLU(0.1),\n",
    "                )\n",
    "            ]),\n",
    "            Residual(PreNorm(32, LinearAttention(32))),\n",
    "            nn.AdaptiveAvgPool2d((4, 4)),\n",
    "            nn.Conv2d(32, 1, 4)\n",
    "        )\n",
    "\n",
    "        self.decoder1 = SimpleDecoder(chan_in = last_chan, chan_out = init_channel)\n",
    "        self.decoder2 = SimpleDecoder(chan_in = features[-2][-1], chan_out = init_channel) if resolution >= 9 else None\n",
    "\n",
    "    def forward(self, x, calc_aux_loss = False):\n",
    "        orig_img = x\n",
    "\n",
    "        for layer in self.non_residual_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        layer_outputs = []\n",
    "\n",
    "        for (net, attn) in self.residual_layers:\n",
    "            if exists(attn):\n",
    "                x = attn(x) + x\n",
    "\n",
    "            x = net(x)\n",
    "            layer_outputs.append(x)\n",
    "\n",
    "        out = self.to_logits(x).flatten(1)\n",
    "\n",
    "        img_32x32 = F.interpolate(orig_img, size = (32, 32))\n",
    "        out_32x32 = self.to_shape_disc_out(img_32x32)\n",
    "\n",
    "        if not calc_aux_loss:\n",
    "            return out, out_32x32, None\n",
    "\n",
    "        # self-supervised auto-encoding loss\n",
    "\n",
    "        layer_8x8 = layer_outputs[-1]\n",
    "        layer_16x16 = layer_outputs[-2]\n",
    "\n",
    "        recon_img_8x8 = self.decoder1(layer_8x8)\n",
    "\n",
    "        aux_loss = F.mse_loss(\n",
    "            recon_img_8x8,\n",
    "            F.interpolate(orig_img, size = recon_img_8x8.shape[2:])\n",
    "        )\n",
    "\n",
    "        if exists(self.decoder2):\n",
    "            select_random_quadrant = lambda rand_quadrant, img: rearrange(img, 'b c (m h) (n w) -> (m n) b c h w', m = 2, n = 2)[rand_quadrant]\n",
    "            crop_image_fn = partial(select_random_quadrant, floor(random() * 4))\n",
    "            img_part, layer_16x16_part = map(crop_image_fn, (orig_img, layer_16x16))\n",
    "\n",
    "            recon_img_16x16 = self.decoder2(layer_16x16_part)\n",
    "\n",
    "            aux_loss_16x16 = F.mse_loss(\n",
    "                recon_img_16x16,\n",
    "                F.interpolate(img_part, size = recon_img_16x16.shape[2:])\n",
    "            )\n",
    "\n",
    "            aux_loss = aux_loss + aux_loss_16x16\n",
    "\n",
    "        return out, out_32x32, aux_loss\n",
    "\n",
    "class LightweightGAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        latent_dim,\n",
    "        image_size,\n",
    "        optimizer = \"adam\",\n",
    "        fmap_max = 256,\n",
    "        fmap_inverse_coef = 10,\n",
    "        disc_output_size = 5,\n",
    "        attn_res_layers = [],\n",
    "        freq_chan_attn = False,\n",
    "        ttur_mult = 1.,\n",
    "        lr = 2e-4,\n",
    "        rank = 0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.image_size = image_size\n",
    "\n",
    "        G_kwargs = dict(\n",
    "            image_size = image_size,\n",
    "            latent_dim = latent_dim,\n",
    "            fmap_max = fmap_max,\n",
    "            fmap_inverse_coef = fmap_inverse_coef,\n",
    "            attn_res_layers = attn_res_layers,\n",
    "            freq_chan_attn = freq_chan_attn\n",
    "        )\n",
    "\n",
    "        self.G = Generator(**G_kwargs)\n",
    "\n",
    "        self.D = Discriminator(\n",
    "            image_size = image_size,\n",
    "            fmap_max = fmap_max,\n",
    "            fmap_inverse_coef = fmap_inverse_coef,\n",
    "            attn_res_layers = attn_res_layers,\n",
    "            disc_output_size = disc_output_size\n",
    "        )\n",
    "\n",
    "        self.ema_updater = EMA(0.995)\n",
    "        self.GE = Generator(**G_kwargs)\n",
    "        set_requires_grad(self.GE, False)\n",
    "\n",
    "\n",
    "        if optimizer == \"adam\":\n",
    "            self.G_opt = Adam(self.G.parameters(), lr = lr, betas=(0.5, 0.9))\n",
    "            self.D_opt = Adam(self.D.parameters(), lr = lr * ttur_mult, betas=(0.5, 0.9))\n",
    "        elif optimizer == \"adabelief\":\n",
    "            self.G_opt = AdaBelief(self.G.parameters(), lr = lr, betas=(0.5, 0.9))\n",
    "            self.D_opt = AdaBelief(self.D.parameters(), lr = lr * ttur_mult, betas=(0.5, 0.9))\n",
    "        else:\n",
    "            assert False, \"No valid optimizer is given\"\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.reset_parameter_averaging()\n",
    "\n",
    "        self.cuda(rank)\n",
    "        self.D_aug = AugWrapper(self.D, image_size)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if type(m) in {nn.Conv2d, nn.Linear}:\n",
    "            nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def EMA(self):\n",
    "        def update_moving_average(ma_model, current_model):\n",
    "            for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "                old_weight, up_weight = ma_params.data, current_params.data\n",
    "                ma_params.data = self.ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "            for current_buffer, ma_buffer in zip(current_model.buffers(), ma_model.buffers()):\n",
    "                new_buffer_value = self.ema_updater.update_average(ma_buffer, current_buffer)\n",
    "                ma_buffer.copy_(new_buffer_value)\n",
    "\n",
    "        update_moving_average(self.GE, self.G)\n",
    "\n",
    "    def reset_parameter_averaging(self):\n",
    "        self.GE.load_state_dict(self.G.state_dict())\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplemented\n",
    "\n",
    "# trainer\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        name = 'default',\n",
    "        results_dir = 'results',\n",
    "        models_dir = 'models',\n",
    "        base_dir = './',\n",
    "        optimizer = 'adam',\n",
    "        num_workers = None,\n",
    "        latent_dim = 256,\n",
    "        image_size = 128,\n",
    "        num_image_tiles = 8,\n",
    "        fmap_max = 512,\n",
    "        batch_size = 64,\n",
    "        gp_weight = 10,\n",
    "        gradient_accumulate_every = 1,\n",
    "        attn_res_layers = [],\n",
    "        freq_chan_attn = False,\n",
    "        disc_output_size = 5,\n",
    "        dual_contrast_loss = False,\n",
    "        antialias = False,\n",
    "        lr = 2e-4,\n",
    "        ttur_mult = 1.,\n",
    "        save_every = 1000,\n",
    "        evaluate_every = 1000,\n",
    "        aug_prob = None,\n",
    "        aug_types = ['translation', 'cutout'],\n",
    "        dataset_aug_prob = 0.,\n",
    "        rank = 0,\n",
    "        amp = False,\n",
    "        hparams = None,\n",
    "        load_strict = True,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.GAN_params = [args, kwargs]\n",
    "        self.GAN = None\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "        base_dir = Path(base_dir)\n",
    "        self.base_dir = base_dir\n",
    "        self.results_dir = base_dir / results_dir\n",
    "        self.models_dir = base_dir / models_dir\n",
    "\n",
    "        self.config_path = self.models_dir / name / '.config.json'\n",
    "\n",
    "        assert is_power_of_two(image_size), 'image size must be a power of 2 (64, 128, 256, 512, 1024)'\n",
    "        assert all(map(is_power_of_two, attn_res_layers)), 'resolution layers of attention must all be powers of 2 (16, 32, 64, 128, 256, 512)'\n",
    "\n",
    "        assert not (dual_contrast_loss and disc_output_size > 1), 'discriminator output size cannot be greater than 1 if using dual contrastive loss'\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.num_image_tiles = num_image_tiles\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fmap_max = fmap_max\n",
    "\n",
    "        self.aug_prob = aug_prob\n",
    "        self.aug_types = aug_types\n",
    "\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.num_workers = num_workers\n",
    "        self.ttur_mult = ttur_mult\n",
    "        self.batch_size = batch_size\n",
    "        self.gradient_accumulate_every = gradient_accumulate_every\n",
    "\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "        self.evaluate_every = evaluate_every\n",
    "        self.save_every = save_every\n",
    "        self.steps = 0\n",
    "\n",
    "        self.attn_res_layers = attn_res_layers\n",
    "        self.freq_chan_attn = freq_chan_attn\n",
    "\n",
    "        self.disc_output_size = disc_output_size\n",
    "        self.antialias = antialias\n",
    "\n",
    "        self.dual_contrast_loss = dual_contrast_loss\n",
    "\n",
    "        self.d_loss = 0\n",
    "        self.g_loss = 0\n",
    "        self.last_gp_loss = None\n",
    "        self.last_recon_loss = None\n",
    "\n",
    "        self.init_folders()\n",
    "\n",
    "        self.loader = None\n",
    "        self.dataset_aug_prob = dataset_aug_prob\n",
    "\n",
    "        self.is_main = rank == 0\n",
    "        self.rank = rank\n",
    "\n",
    "\n",
    "        self.load_strict = load_strict\n",
    "\n",
    "        self.amp = amp\n",
    "        self.G_scaler = GradScaler(enabled = self.amp)\n",
    "        self.D_scaler = GradScaler(enabled = self.amp)\n",
    "\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.image_extension = 'jpg'\n",
    "\n",
    "    @property\n",
    "    def checkpoint_num(self):\n",
    "        return floor(self.steps // self.save_every)\n",
    "        \n",
    "    def init_GAN(self):\n",
    "        args, kwargs = self.GAN_params\n",
    "\n",
    "        # set some global variables before instantiating GAN\n",
    "\n",
    "        global norm_class\n",
    "        global Blur\n",
    "\n",
    "        Blur = nn.Identity if not self.antialias else Blur\n",
    "\n",
    "        # instantiate GAN\n",
    "\n",
    "        self.GAN = LightweightGAN(\n",
    "            optimizer=self.optimizer,\n",
    "            lr = self.lr,\n",
    "            latent_dim = self.latent_dim,\n",
    "            attn_res_layers = self.attn_res_layers,\n",
    "            freq_chan_attn = self.freq_chan_attn,\n",
    "            image_size = self.image_size,\n",
    "            ttur_mult = self.ttur_mult,\n",
    "            fmap_max = self.fmap_max,\n",
    "            disc_output_size = self.disc_output_size,\n",
    "            rank = self.rank,\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def write_config(self):\n",
    "        self.config_path.write_text(json.dumps(self.config()))\n",
    "\n",
    "    def load_config(self):\n",
    "        config = self.config() if not self.config_path.exists() else json.loads(self.config_path.read_text())\n",
    "        self.image_size = config['image_size']\n",
    "        self.disc_output_size = config['disc_output_size']\n",
    "        self.attn_res_layers = config.pop('attn_res_layers', [])\n",
    "        self.freq_chan_attn = config.pop('freq_chan_attn', False)\n",
    "        self.optimizer = config.pop('optimizer', 'adam')\n",
    "        self.fmap_max = config.pop('fmap_max', 256)\n",
    "        del self.GAN\n",
    "        self.init_GAN()\n",
    "\n",
    "    def config(self):\n",
    "        return {\n",
    "            'image_size': self.image_size,\n",
    "            'disc_output_size': self.disc_output_size,\n",
    "            'optimizer': self.optimizer,\n",
    "            'attn_res_layers': self.attn_res_layers,\n",
    "            'freq_chan_attn': self.freq_chan_attn\n",
    "        }\n",
    "\n",
    "    def set_data_src(self, folder):\n",
    "        self.dataset = ImageDataset(folder, self.image_size)\n",
    "        dataloader = DataLoader(self.dataset, batch_size = self.batch_size, shuffle = True, drop_last = True, pin_memory = True)\n",
    "        self.loader = cycle(dataloader)\n",
    "\n",
    "        # auto set augmentation prob for user if dataset is detected to be low\n",
    "        num_samples = len(self.dataset)\n",
    "        if not exists(self.aug_prob) and num_samples < 1e5:\n",
    "            self.aug_prob = min(0.5, (1e5 - num_samples) * 3e-6)\n",
    "            print(f'autosetting augmentation probability to {round(self.aug_prob * 100)}%')\n",
    "\n",
    "    def train(self):\n",
    "        assert exists(self.loader), 'You must first initialize the data source with `.set_data_src(<folder of images>)`'\n",
    "        device = torch.device(f'cuda:{self.rank}')\n",
    "\n",
    "        if not exists(self.GAN):\n",
    "            self.init_GAN()\n",
    "\n",
    "        self.GAN.train()\n",
    "        total_disc_loss = torch.zeros([], device=device)\n",
    "        total_gen_loss = torch.zeros([], device=device)\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        latent_dim = self.GAN.latent_dim\n",
    "\n",
    "        aug_prob   = default(self.aug_prob, 0)\n",
    "        aug_types  = self.aug_types\n",
    "        aug_kwargs = {'prob': aug_prob, 'types': aug_types}\n",
    "\n",
    "        G = self.GAN.G\n",
    "        D = self.GAN.D\n",
    "        D_aug = self.GAN.D_aug\n",
    "\n",
    "        apply_gradient_penalty = self.steps % 4 == 0\n",
    "\n",
    "        # amp related contexts and functions\n",
    "\n",
    "        amp_context = autocast if self.amp else null_context\n",
    "\n",
    "        # discriminator loss fn\n",
    "\n",
    "        if self.dual_contrast_loss:\n",
    "            D_loss_fn = dual_contrastive_loss\n",
    "        else:\n",
    "            D_loss_fn = hinge_loss\n",
    "\n",
    "        # train discriminator\n",
    "\n",
    "        self.GAN.D_opt.zero_grad()\n",
    "\n",
    "        latents = torch.randn(batch_size, latent_dim).cuda(self.rank)\n",
    "        image_batch = next(self.loader).cuda(self.rank)\n",
    "        image_batch.requires_grad_()\n",
    "\n",
    "        with amp_context():\n",
    "            with torch.no_grad():\n",
    "                generated_images = G(latents)\n",
    "\n",
    "            fake_output, fake_output_32x32, _ = D_aug(generated_images, detach = True, **aug_kwargs)\n",
    "\n",
    "            real_output, real_output_32x32, real_aux_loss = D_aug(image_batch,  calc_aux_loss = True, **aug_kwargs)\n",
    "\n",
    "            real_output_loss = real_output\n",
    "            fake_output_loss = fake_output\n",
    "\n",
    "            divergence = D_loss_fn(real_output_loss, fake_output_loss)\n",
    "            divergence_32x32 = D_loss_fn(real_output_32x32, fake_output_32x32)\n",
    "            disc_loss = divergence + divergence_32x32\n",
    "\n",
    "            aux_loss = real_aux_loss\n",
    "            disc_loss = disc_loss + aux_loss\n",
    "\n",
    "        if apply_gradient_penalty:\n",
    "            outputs = [real_output, real_output_32x32]\n",
    "            outputs = list(map(self.D_scaler.scale, outputs)) if self.amp else outputs\n",
    "\n",
    "            scaled_gradients = torch_grad(outputs=outputs, inputs=image_batch,\n",
    "                                    grad_outputs=list(map(lambda t: torch.ones(t.size(), device = image_batch.device), outputs)),\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "            inv_scale = safe_div(1., self.D_scaler.get_scale()) if self.amp else 1.\n",
    "\n",
    "            if inv_scale != float('inf'):\n",
    "                gradients = scaled_gradients * inv_scale\n",
    "\n",
    "                with amp_context():\n",
    "                    gradients = gradients.reshape(batch_size, -1)\n",
    "                    gp =  self.gp_weight * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "                    if not torch.isnan(gp):\n",
    "                        disc_loss = disc_loss + gp\n",
    "                        self.last_gp_loss = gp.clone().detach().item()\n",
    "\n",
    "        with amp_context():\n",
    "            disc_loss = disc_loss / self.gradient_accumulate_every\n",
    "\n",
    "        disc_loss.register_hook(raise_if_nan)\n",
    "        self.D_scaler.scale(disc_loss).backward()\n",
    "        total_disc_loss += divergence\n",
    "\n",
    "        self.last_recon_loss = aux_loss.item()\n",
    "        self.d_loss = float(total_disc_loss.item() / self.gradient_accumulate_every)\n",
    "        self.D_scaler.step(self.GAN.D_opt)\n",
    "        self.D_scaler.update()\n",
    "\n",
    "        # generator loss fn\n",
    "\n",
    "        if self.dual_contrast_loss:\n",
    "            G_loss_fn = dual_contrastive_loss\n",
    "            G_requires_calc_real = True\n",
    "        else:\n",
    "            G_loss_fn = gen_hinge_loss\n",
    "            G_requires_calc_real = False\n",
    "\n",
    "        # train generator\n",
    "\n",
    "        self.GAN.G_opt.zero_grad()\n",
    "\n",
    "        latents = torch.randn(batch_size, latent_dim).cuda(self.rank)\n",
    "\n",
    "        if G_requires_calc_real:\n",
    "            image_batch = next(self.loader).cuda(self.rank)\n",
    "            image_batch.requires_grad_()\n",
    "\n",
    "        with amp_context():\n",
    "            generated_images = G(latents)\n",
    "\n",
    "            fake_output, fake_output_32x32, _ = D_aug(generated_images, **aug_kwargs)\n",
    "            real_output, real_output_32x32, _ = D_aug(image_batch, **aug_kwargs) if G_requires_calc_real else (None, None, None)\n",
    "\n",
    "            loss = G_loss_fn(fake_output, real_output)\n",
    "            loss_32x32 = G_loss_fn(fake_output_32x32, real_output_32x32)\n",
    "\n",
    "            gen_loss = loss + loss_32x32\n",
    "\n",
    "            gen_loss = gen_loss / self.gradient_accumulate_every\n",
    "\n",
    "        gen_loss.register_hook(raise_if_nan)\n",
    "        self.G_scaler.scale(gen_loss).backward()\n",
    "        total_gen_loss += loss\n",
    "\n",
    "        self.g_loss = float(total_gen_loss.item() / self.gradient_accumulate_every)\n",
    "        self.G_scaler.step(self.GAN.G_opt)\n",
    "        self.G_scaler.update()\n",
    "\n",
    "        # calculate moving averages\n",
    "\n",
    "        if self.is_main and self.steps % 10 == 0 and self.steps > 20000:\n",
    "            self.GAN.EMA()\n",
    "\n",
    "        if self.is_main and self.steps <= 25000 and self.steps % 1000 == 2:\n",
    "            self.GAN.reset_parameter_averaging()\n",
    "\n",
    "        # save from NaN errors\n",
    "\n",
    "        if any(torch.isnan(l) for l in (total_gen_loss, total_disc_loss)):\n",
    "            print(f'NaN detected for generator or discriminator. Loading from checkpoint #{self.checkpoint_num}')\n",
    "            self.load(self.checkpoint_num)\n",
    "            raise NanException\n",
    "\n",
    "        del total_disc_loss\n",
    "        del total_gen_loss\n",
    "\n",
    "        # periodically save results\n",
    "\n",
    "        if self.is_main:\n",
    "            if self.steps % self.save_every == 0:\n",
    "                self.save(self.checkpoint_num)\n",
    "\n",
    "            if self.steps % self.evaluate_every == 0 or (self.steps % 100 == 0 and self.steps < 20000):\n",
    "                self.evaluate(floor(self.steps / self.evaluate_every), num_image_tiles = self.num_image_tiles)\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, num = 0, num_image_tiles = 4):\n",
    "        self.GAN.eval()\n",
    "\n",
    "        ext = self.image_extension\n",
    "        num_rows = num_image_tiles\n",
    "    \n",
    "        latent_dim = self.GAN.latent_dim\n",
    "\n",
    "        latents = det_randn((num_rows ** 2, latent_dim)).cuda(self.rank)\n",
    "        interpolate_latents = interpolate_between(latents[:num_rows], latents[-num_rows:],\n",
    "                                                  num_samples=num_rows,\n",
    "                                                  dim=0).flatten(end_dim=1)\n",
    "\n",
    "        generate_interpolations = self.generate_(self.GAN.G, interpolate_latents)\n",
    "        torchvision.utils.save_image(generate_interpolations, str(self.results_dir / self.name / f'{str(num)}-interp.{ext}'), nrow=num_rows)\n",
    "        # regular\n",
    "\n",
    "        generated_images = self.generate_(self.GAN.G, latents)\n",
    "\n",
    "        torchvision.utils.save_image(generated_images, str(self.results_dir / self.name / f'{str(num)}.{ext}'), nrow=num_rows)\n",
    "\n",
    "        # moving averages\n",
    "\n",
    "        generated_images = self.generate_(self.GAN.GE, latents)\n",
    "        torchvision.utils.save_image(generated_images, str(self.results_dir / self.name / f'{str(num)}-ema.{ext}'), nrow=num_rows)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, num=0, num_image_tiles=4, checkpoint=None, types=['default', 'ema']):\n",
    "        self.GAN.eval()\n",
    "\n",
    "        latent_dim = self.GAN.latent_dim\n",
    "        dir_name = self.name + str('-generated-') + str(checkpoint)\n",
    "        dir_full = Path().absolute() / self.results_dir / dir_name\n",
    "        ext = self.image_extension\n",
    "\n",
    "        if not dir_full.exists():\n",
    "            os.mkdir(dir_full)\n",
    "\n",
    "        # regular\n",
    "        if 'default' in types:\n",
    "            for i in tqdm(range(num_image_tiles), desc='Saving generated default images'):\n",
    "                latents = torch.randn((1, latent_dim)).cuda(self.rank)\n",
    "                generated_image = self.generate_(self.GAN.G, latents)\n",
    "                path = str(self.results_dir / dir_name / f'{str(num)}-{str(i)}.{ext}')\n",
    "                torchvision.utils.save_image(generated_image[0], path, nrow=1)\n",
    "\n",
    "        # moving averages\n",
    "        if 'ema' in types:\n",
    "            for i in tqdm(range(num_image_tiles), desc='Saving generated EMA images'):\n",
    "                latents = torch.randn((1, latent_dim)).cuda(self.rank)\n",
    "                generated_image = self.generate_(self.GAN.GE, latents)\n",
    "                path = str(self.results_dir / dir_name / f'{str(num)}-{str(i)}-ema.{ext}')\n",
    "                torchvision.utils.save_image(generated_image[0], path, nrow=1)\n",
    "\n",
    "        return dir_full\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def show_progress(self, num_images=4, types=['default', 'ema']):\n",
    "        checkpoints = self.get_checkpoints()\n",
    "        assert exists(checkpoints), 'cannot find any checkpoints to create a training progress video for'\n",
    "\n",
    "        dir_name = self.name + str('-progress')\n",
    "        dir_full = Path().absolute() / self.results_dir / dir_name\n",
    "        ext = self.image_extension\n",
    "        latents = None\n",
    "\n",
    "        zfill_length = math.ceil(math.log10(len(checkpoints)))\n",
    "\n",
    "        if not dir_full.exists():\n",
    "            os.mkdir(dir_full)\n",
    "\n",
    "        for checkpoint in tqdm(checkpoints, desc='Generating progress images'):\n",
    "            self.load(checkpoint, print_version=False)\n",
    "            self.GAN.eval()\n",
    "\n",
    "            if checkpoint == 0:\n",
    "                latents = torch.randn((num_images, self.GAN.latent_dim)).cuda(self.rank)\n",
    "\n",
    "            # regular\n",
    "            if 'default' in types:\n",
    "                generated_image = self.generate_(self.GAN.G, latents)\n",
    "                path = str(self.results_dir / dir_name / f'{str(checkpoint).zfill(zfill_length)}.{ext}')\n",
    "                torchvision.utils.save_image(generated_image, path, nrow=num_images)\n",
    "\n",
    "            # moving averages\n",
    "            if 'ema' in types:\n",
    "                generated_image = self.generate_(self.GAN.GE, latents)\n",
    "                path = str(self.results_dir / dir_name / f'{str(checkpoint).zfill(zfill_length)}-ema.{ext}')\n",
    "                torchvision.utils.save_image(generated_image, path, nrow=num_images)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_(self, G, style, num_image_tiles = 8):\n",
    "        generated_images = evaluate_in_chunks(self.batch_size, G, style)\n",
    "        return generated_images.clamp_(0., 1.)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_interpolation(self, num = 0, num_image_tiles = 8, num_steps = 100, save_frames = False):\n",
    "        self.GAN.eval()\n",
    "        ext = self.image_extension\n",
    "        num_rows = num_image_tiles\n",
    "\n",
    "        latent_dim = self.GAN.latent_dim\n",
    "\n",
    "        # latents and noise\n",
    "\n",
    "        latents_low = torch.randn(num_rows ** 2, latent_dim).cuda(self.rank)\n",
    "        latents_high = torch.randn(num_rows ** 2, latent_dim).cuda(self.rank)\n",
    "\n",
    "        ratios = torch.linspace(0., 8., num_steps)\n",
    "\n",
    "        frames = []\n",
    "        for ratio in tqdm(ratios):\n",
    "            interp_latents = slerp(ratio, latents_low, latents_high)\n",
    "            generated_images = self.generate_(self.GAN.GE, interp_latents)\n",
    "            images_grid = torchvision.utils.make_grid(generated_images, nrow = num_rows)\n",
    "            pil_image = transforms.ToPILImage()(images_grid.cpu())\n",
    "                \n",
    "            frames.append(pil_image)\n",
    "\n",
    "        frames[0].save(str(self.results_dir / self.name / f'{str(num)}.gif'), save_all=True, append_images=frames[1:], duration=80, loop=0, optimize=True)\n",
    "\n",
    "        if save_frames:\n",
    "            folder_path = (self.results_dir / self.name / f'{str(num)}')\n",
    "            folder_path.mkdir(parents=True, exist_ok=True)\n",
    "            for ind, frame in enumerate(frames):\n",
    "                frame.save(str(folder_path / f'{str(ind)}.{ext}'))\n",
    "\n",
    "    def print_log(self):\n",
    "        data = [\n",
    "            ('G', self.g_loss),\n",
    "            ('D', self.d_loss),\n",
    "            ('GP', self.last_gp_loss),\n",
    "            ('SS', self.last_recon_loss),\n",
    "        ]\n",
    "\n",
    "        data = [d for d in data if exists(d[1])]\n",
    "        log = ' | '.join(map(lambda n: f'{n[0]}: {n[1]:.2f}', data))\n",
    "        print(log)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def model_name(self, num):\n",
    "        return str(self.models_dir / self.name / f'model_{num}.pt')\n",
    "\n",
    "    def init_folders(self):\n",
    "        (self.results_dir / self.name).mkdir(parents=True, exist_ok=True)\n",
    "        (self.models_dir / self.name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def clear(self):\n",
    "        rmtree(str(self.models_dir / self.name), True)\n",
    "        rmtree(str(self.results_dir / self.name), True)\n",
    "        rmtree(str(self.config_path), True)\n",
    "        self.init_folders()\n",
    "\n",
    "    def save(self, num):\n",
    "        save_data = {\n",
    "            'GAN': self.GAN.state_dict(),\n",
    "            'G_scaler': self.G_scaler.state_dict(),\n",
    "            'D_scaler': self.D_scaler.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(save_data, self.model_name(num))\n",
    "        self.write_config()\n",
    "\n",
    "    def load(self, num=-1, print_version=True):\n",
    "        self.load_config()\n",
    "\n",
    "        name = num\n",
    "        if num == -1:\n",
    "            checkpoints = self.get_checkpoints()\n",
    "\n",
    "            if not exists(checkpoints):\n",
    "                return\n",
    "\n",
    "            name = checkpoints[-1]\n",
    "            print(f'continuing from previous epoch - {name}')\n",
    "\n",
    "        self.steps = name * self.save_every\n",
    "\n",
    "        load_data = torch.load(self.model_name(name))\n",
    "\n",
    "        if print_version and 'version' in load_data and self.is_main:\n",
    "            print(f\"loading from version {load_data['version']}\")\n",
    "\n",
    "        try:\n",
    "            self.GAN.load_state_dict(load_data['GAN'], strict = self.load_strict)\n",
    "        except Exception as e:\n",
    "            print('unable to load save model.')\n",
    "            raise e\n",
    "\n",
    "        if 'G_scaler' in load_data:\n",
    "            self.G_scaler.load_state_dict(load_data['G_scaler'])\n",
    "        if 'D_scaler' in load_data:\n",
    "            self.D_scaler.load_state_dict(load_data['D_scaler'])\n",
    "\n",
    "    def get_checkpoints(self):\n",
    "        file_paths = [p for p in Path(self.models_dir / self.name).glob('model_*.pt')]\n",
    "        saved_nums = sorted(map(lambda x: int(x.stem.split('_')[1]), file_paths))\n",
    "\n",
    "        if len(saved_nums) == 0:\n",
    "            return None\n",
    "\n",
    "        return saved_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def cast_list(el):\n",
    "    return el if isinstance(el, list) else [el]\n",
    "\n",
    "def timestamped_filename(prefix = 'generated-'):\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    return f'{prefix}{timestamp}'\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    r_seed(seed)\n",
    "\n",
    "def run_training(rank, model_args, data, load_from, new, num_train_steps, name, seed):\n",
    "    is_main = rank == 0\n",
    "\n",
    "    model_args.update(\n",
    "        rank = rank,\n",
    "    )\n",
    "\n",
    "    model = Trainer(**model_args, hparams=model_args)\n",
    "\n",
    "    if not new:\n",
    "        model.load(load_from)\n",
    "    else:\n",
    "        model.clear()\n",
    "\n",
    "    model.set_data_src(data)\n",
    "\n",
    "    progress_bar = tqdm(initial = model.steps, total = num_train_steps, mininterval=10., desc=f'{name}<{data}>')\n",
    "    while model.steps < num_train_steps:\n",
    "        retry_call(model.train, tries=3, exceptions=NanException)\n",
    "        progress_bar.n = model.steps\n",
    "        progress_bar.refresh()\n",
    "        if is_main and model.steps % 50 == 0:\n",
    "            model.print_log()\n",
    "\n",
    "    model.save(model.checkpoint_num)\n",
    "\n",
    "\n",
    "def train_from_folder(\n",
    "    data = './images',\n",
    "    results_dir = './results',\n",
    "    models_dir = './models',\n",
    "    name = 'default',\n",
    "    new = False,\n",
    "    load_from = -1,\n",
    "    image_size = 128,\n",
    "    optimizer = 'adam',\n",
    "    fmap_max = 256,\n",
    "    batch_size = 64,\n",
    "    gradient_accumulate_every = 4,\n",
    "    num_train_steps = 150000,\n",
    "    learning_rate = 2e-4,\n",
    "    save_every = 1000,\n",
    "    evaluate_every = 1000,\n",
    "    generate = False,\n",
    "    generate_types = ['default', 'ema'],\n",
    "    generate_interpolation = False,\n",
    "    aug_prob=None,\n",
    "    aug_types=['cutout', 'translation'],\n",
    "    dataset_aug_prob=0.,\n",
    "    attn_res_layers = [],\n",
    "    freq_chan_attn = False,\n",
    "    disc_output_size = 1,\n",
    "    dual_contrast_loss = False,\n",
    "    antialias = False,\n",
    "    interpolation_num_steps = 100,\n",
    "    save_frames = False,\n",
    "    num_image_tiles = None,\n",
    "    num_workers = None,\n",
    "    seed = 42,\n",
    "    amp = False,\n",
    "    show_progress = False,\n",
    "    load_strict = True\n",
    "):\n",
    "    num_image_tiles = default(num_image_tiles, 4 if image_size > 512 else 8)\n",
    "\n",
    "    model_args = dict(\n",
    "        name = name,\n",
    "        results_dir = results_dir,\n",
    "        models_dir = models_dir,\n",
    "        batch_size = batch_size,\n",
    "        gradient_accumulate_every = gradient_accumulate_every,\n",
    "        attn_res_layers = cast_list(attn_res_layers),\n",
    "        freq_chan_attn = freq_chan_attn,\n",
    "        disc_output_size = disc_output_size,\n",
    "        dual_contrast_loss = dual_contrast_loss,\n",
    "        antialias = antialias,\n",
    "        image_size = image_size,\n",
    "        num_image_tiles = num_image_tiles,\n",
    "        optimizer = optimizer,\n",
    "        num_workers = num_workers,\n",
    "        fmap_max = fmap_max,\n",
    "        lr = learning_rate,\n",
    "        save_every = save_every,\n",
    "        evaluate_every = evaluate_every,\n",
    "        aug_prob = aug_prob,\n",
    "        aug_types = cast_list(aug_types),\n",
    "        dataset_aug_prob = dataset_aug_prob,\n",
    "        amp = amp,\n",
    "        load_strict = load_strict\n",
    "    )\n",
    "\n",
    "    if generate:\n",
    "        model = Trainer(**model_args)\n",
    "        model.load(load_from)\n",
    "        samples_name = timestamped_filename()\n",
    "        checkpoint = model.checkpoint_num\n",
    "        dir_result = model.generate(samples_name, num_image_tiles, checkpoint, generate_types)\n",
    "        print(f'sample images generated at {dir_result}')\n",
    "        return\n",
    "\n",
    "    if generate_interpolation:\n",
    "        model = Trainer(**model_args)\n",
    "        model.load(load_from)\n",
    "        samples_name = timestamped_filename()\n",
    "        model.generate_interpolation(samples_name, num_image_tiles, num_steps = interpolation_num_steps, save_frames = save_frames)\n",
    "        print(f'interpolation generated at {results_dir}/{name}/{samples_name}')\n",
    "        return\n",
    "\n",
    "    if show_progress:\n",
    "        model = Trainer(**model_args)\n",
    "        model.show_progress(num_images=num_image_tiles, types=generate_types)\n",
    "        return\n",
    "\n",
    "\n",
    "    run_training(0, model_args, data, load_from, new, num_train_steps, name, seed)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autosetting augmentation probability to 9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 50/150000 [00:27<22:52:01,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.64 | D: 0.00 | GP: 1.86 | SS: 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 100/150000 [00:52<21:56:23,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.50 | D: 0.08 | GP: 1.39 | SS: 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 150/150000 [01:17<21:33:49,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.35 | D: 0.43 | GP: 0.60 | SS: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 200/150000 [01:42<21:24:41,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.25 | D: 0.55 | GP: 0.52 | SS: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 250/150000 [02:07<21:17:18,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.28 | D: 0.58 | GP: 0.79 | SS: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 300/150000 [02:33<21:14:43,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.30 | D: 0.44 | GP: 0.45 | SS: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 350/150000 [02:58<21:12:14,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.16 | D: 0.43 | GP: 0.30 | SS: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 400/150000 [03:23<21:09:43,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.14 | D: 0.50 | GP: 0.42 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 450/150000 [03:49<21:11:17,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.44 | D: 0.44 | GP: 0.21 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 500/150000 [04:14<21:10:36,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.10 | D: 0.51 | GP: 0.10 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 550/150000 [04:40<21:11:12,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.03 | D: 0.45 | GP: 0.32 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 600/150000 [05:05<21:09:35,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.30 | D: 0.62 | GP: 0.27 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 650/150000 [05:31<21:09:57,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.06 | D: 0.60 | GP: 0.25 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 700/150000 [05:57<21:09:03,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.23 | D: 0.29 | GP: 0.34 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   0%|          | 750/150000 [06:22<21:09:30,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -0.41 | D: 0.45 | GP: 0.50 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 800/150000 [06:47<21:06:30,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.20 | D: 0.39 | GP: 0.30 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 850/150000 [07:12<21:03:57,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.06 | D: 0.66 | GP: 0.26 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 900/150000 [07:36<21:01:46,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.17 | D: 0.43 | GP: 0.37 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 950/150000 [08:02<21:02:14,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -0.03 | D: 0.55 | GP: 0.10 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1000/150000 [08:27<21:01:20,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.10 | D: 0.51 | GP: 0.34 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1050/150000 [08:53<21:01:36,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.16 | D: 0.45 | GP: 0.38 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1100/150000 [09:18<21:00:27,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -0.04 | D: 0.54 | GP: 0.25 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1150/150000 [09:42<20:56:02,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.12 | D: 0.51 | GP: 0.34 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1200/150000 [10:05<20:51:33,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.13 | D: 0.42 | GP: 0.40 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1250/150000 [10:29<20:48:06,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.14 | D: 0.49 | GP: 0.20 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1300/150000 [10:53<20:44:57,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -0.04 | D: 0.52 | GP: 0.71 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1350/150000 [11:17<20:43:05,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -0.04 | D: 0.54 | GP: 0.73 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1400/150000 [11:40<20:39:18,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.16 | D: 0.57 | GP: 0.50 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1450/150000 [12:04<20:36:30,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.11 | D: 0.49 | GP: 0.58 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1500/150000 [12:27<20:33:19,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.57 | GP: 0.83 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1550/150000 [12:51<20:30:43,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.12 | D: 0.53 | GP: 0.90 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1600/150000 [13:14<20:28:04,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.06 | D: 0.48 | GP: 0.70 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1650/150000 [13:38<20:26:03,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.14 | D: 0.60 | GP: 0.37 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1700/150000 [14:01<20:23:12,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.29 | D: 0.54 | GP: 0.38 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1750/150000 [14:25<20:21:19,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.02 | D: 0.52 | GP: 0.64 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1800/150000 [14:48<20:19:22,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -0.09 | D: 0.39 | GP: 0.70 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|          | 1850/150000 [15:12<20:17:46,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.18 | D: 0.56 | GP: 0.94 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|▏         | 1900/150000 [15:35<20:15:32,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.05 | D: 0.50 | GP: 0.33 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|▏         | 1950/150000 [15:59<20:13:49,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.07 | D: 0.51 | GP: 0.65 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|▏         | 2000/150000 [16:22<20:11:47,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.15 | D: 0.46 | GP: 0.51 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|▏         | 2050/150000 [16:46<20:10:22,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -0.04 | D: 0.54 | GP: 0.27 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|▏         | 2100/150000 [17:09<20:08:15,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.12 | D: 0.50 | GP: 0.57 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|▏         | 2150/150000 [17:33<20:07:18,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -0.08 | D: 0.62 | GP: 0.55 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   1%|▏         | 2200/150000 [17:57<20:05:59,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.16 | D: 0.52 | GP: 0.31 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2250/150000 [18:21<20:05:13,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.18 | D: 0.49 | GP: 0.34 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2300/150000 [18:44<20:04:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.09 | D: 0.57 | GP: 0.25 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2350/150000 [19:09<20:03:13,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.19 | D: 0.53 | GP: 0.34 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2400/150000 [19:32<20:02:04,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.17 | D: 0.52 | GP: 0.30 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2450/150000 [19:57<20:01:44,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.20 | D: 0.50 | GP: 0.46 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2500/150000 [20:21<20:01:02,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -0.05 | D: 0.48 | GP: 0.29 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2550/150000 [20:45<20:00:47,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.30 | D: 0.53 | GP: 0.18 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2600/150000 [21:10<20:00:01,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.09 | D: 0.46 | GP: 0.46 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2650/150000 [21:34<19:59:42,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.11 | D: 0.46 | GP: 1.46 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2700/150000 [21:58<19:59:01,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.14 | D: 0.53 | GP: 0.17 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2750/150000 [22:23<19:58:48,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.11 | D: 0.36 | GP: 0.17 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2800/150000 [22:47<19:58:04,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.11 | D: 0.52 | GP: 0.16 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2850/150000 [23:11<19:57:46,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.04 | D: 0.47 | GP: 0.17 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2900/150000 [23:36<19:57:12,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.04 | D: 0.49 | GP: 0.12 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 2950/150000 [24:00<19:56:50,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.20 | D: 0.52 | GP: 0.28 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 3000/150000 [24:24<19:56:13,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.05 | D: 0.42 | GP: 0.27 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 3050/150000 [24:49<19:55:49,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.03 | D: 0.48 | GP: 0.13 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 3100/150000 [25:13<19:55:22,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.14 | D: 0.44 | GP: 0.53 | SS: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "default<./images>:   2%|▏         | 3111/150000 [25:19<19:55:22,  2.05it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_from_folder()\n",
      "Cell \u001b[0;32mIn[3], line 136\u001b[0m, in \u001b[0;36mtrain_from_folder\u001b[0;34m(data, results_dir, models_dir, name, new, load_from, image_size, optimizer, fmap_max, batch_size, gradient_accumulate_every, num_train_steps, learning_rate, save_every, evaluate_every, generate, generate_types, generate_interpolation, aug_prob, aug_types, dataset_aug_prob, attn_res_layers, freq_chan_attn, disc_output_size, dual_contrast_loss, antialias, interpolation_num_steps, save_frames, num_image_tiles, num_workers, seed, amp, show_progress, load_strict)\u001b[0m\n\u001b[1;32m    132\u001b[0m     model\u001b[39m.\u001b[39mshow_progress(num_images\u001b[39m=\u001b[39mnum_image_tiles, types\u001b[39m=\u001b[39mgenerate_types)\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m run_training(\u001b[39m0\u001b[39;49m, model_args, data, load_from, new, num_train_steps, name, seed)\n\u001b[1;32m    137\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(rank, model_args, data, load_from, new, num_train_steps, name, seed)\u001b[0m\n\u001b[1;32m     38\u001b[0m progress_bar \u001b[39m=\u001b[39m tqdm(initial \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39msteps, total \u001b[39m=\u001b[39m num_train_steps, mininterval\u001b[39m=\u001b[39m\u001b[39m10.\u001b[39m, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m<\u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m}\u001b[39;00m\u001b[39m>\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[39mwhile\u001b[39;00m model\u001b[39m.\u001b[39msteps \u001b[39m<\u001b[39m num_train_steps:\n\u001b[0;32m---> 40\u001b[0m     retry_call(model\u001b[39m.\u001b[39;49mtrain, tries\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, exceptions\u001b[39m=\u001b[39;49mNanException)\n\u001b[1;32m     41\u001b[0m     progress_bar\u001b[39m.\u001b[39mn \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39msteps\n\u001b[1;32m     42\u001b[0m     progress_bar\u001b[39m.\u001b[39mrefresh()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/retry/api.py:101\u001b[0m, in \u001b[0;36mretry_call\u001b[0;34m(f, fargs, fkwargs, exceptions, tries, delay, max_delay, backoff, jitter, logger)\u001b[0m\n\u001b[1;32m     99\u001b[0m args \u001b[39m=\u001b[39m fargs \u001b[39mif\u001b[39;00m fargs \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m()\n\u001b[1;32m    100\u001b[0m kwargs \u001b[39m=\u001b[39m fkwargs \u001b[39mif\u001b[39;00m fkwargs \u001b[39melse\u001b[39;00m \u001b[39mdict\u001b[39m()\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m __retry_internal(partial(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), exceptions, tries, delay, max_delay, backoff, jitter, logger)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/retry/api.py:33\u001b[0m, in \u001b[0;36m__retry_internal\u001b[0;34m(f, exceptions, tries, delay, max_delay, backoff, jitter, logger)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m _tries:\n\u001b[1;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         \u001b[39mreturn\u001b[39;00m f()\n\u001b[1;32m     34\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     35\u001b[0m         _tries \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 1118\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mG_scaler\u001b[39m.\u001b[39mscale(gen_loss)\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   1116\u001b[0m total_gen_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m-> 1118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(total_gen_loss\u001b[39m.\u001b[39;49mitem() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_accumulate_every)\n\u001b[1;32m   1119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mG_scaler\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGAN\u001b[39m.\u001b[39mG_opt)\n\u001b[1;32m   1120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mG_scaler\u001b[39m.\u001b[39mupdate()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_from_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
